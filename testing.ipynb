{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\FastAPI Learning\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load the sentiment-analysis model from Hugging Face\n",
    "model_name = \"cardiffnlp/twitter-roberta-base-sentiment\"\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\", model=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'LABEL_1', 'score': 0.6970634460449219}]\n"
     ]
    }
   ],
   "source": [
    "result = sentiment_pipeline(\"I've been waiting for a Hugging Face course my whole life.\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LABEL_1\n"
     ]
    }
   ],
   "source": [
    "print(result[0]['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import io\n",
    "from pydantic import BaseModel\n",
    "\n",
    "# Import all preprocessing functions\n",
    "from scripts.preprocessing import (\n",
    "    handle_missing_values,\n",
    "    drop_columns,\n",
    "    filter_outliers,\n",
    "    feature_encoding,\n",
    "    feature_scaling\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "This is a simple FastAPI app that loads a saved model (from my previous data science project) and uses it to make predictions.\n",
    "\"\"\"\n",
    "\n",
    "# Define request schema\n",
    "class InputData(BaseModel):\n",
    "    Daily_Time_Spent_on_Site: float\n",
    "    Age: float\n",
    "    Area_Income: float\n",
    "    Daily_Internet_Usage: float\n",
    "    Gender: str\n",
    "    Visit_Date: str\n",
    "    City: str\n",
    "    Province: str\n",
    "    Category: str\n",
    "    \n",
    "    class Config:\n",
    "        populate_by_name = True\n",
    "        json_schema_extra = {\n",
    "            \"example\": {\n",
    "                \"Daily_Time_Spent_on_Site\": 68.95,\n",
    "                \"Age\": 35,\n",
    "                \"Area_Income\": 61833.90,\n",
    "                \"Daily_Internet_Usage\": 256.09,\n",
    "                \"Gender\": \"Male\",\n",
    "                \"Visit_Date\": \"2023-01-15\",\n",
    "                \"City\": \"New York\",\n",
    "                \"Province\": \"NY\",\n",
    "                \"Category\": \"Technology\"\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing function\n",
    "def preprocess_data(input_data: InputData):\n",
    "    \"\"\"\n",
    "    Apply the same preprocessing steps used during model training\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert input data to DataFrame\n",
    "    input_df = pd.DataFrame([input_data.model_dump()])\n",
    "    \n",
    "    # Print the input data columns for debugging\n",
    "    print(f\"Original input columns: {input_df.columns}\")\n",
    "\n",
    "    # Handle column name formatting - correct any column name mismatches\n",
    "    column_mapping = {\n",
    "        'Daily_Time_Spent_on_Site': 'Daily Time Spent on Site',\n",
    "        'Area_Income': 'Area Income',\n",
    "        'Daily_Internet_Usage': 'Daily Internet Usage',\n",
    "        'Visit_Date': 'Visit Date'\n",
    "    }\n",
    "\n",
    "        \n",
    "    input_df = input_df.rename(columns=column_mapping)\n",
    "    print(f\"Renamed columns: {input_df.columns}\")\n",
    "\n",
    "        # 1. Drop some uncessary columns\n",
    "    try:    \n",
    "        if 'Visit Time' in input_df.columns:\n",
    "            input_df = drop_columns(input_df, columns=['Visit Time'])\n",
    "    except Exception as e:\n",
    "        print(f\"Error in dropping columns: {str(e)}\")\n",
    "\n",
    "    # 2. Handle missing values\n",
    "    try:\n",
    "        input_df = handle_missing_values(input_df, columns=['Daily Time Spent on Site', 'Daily Internet Usage'], strategy='fill', imputation_method='mean')\n",
    "        input_df['Area Income'] = handle_missing_values(input_df, columns=['Area Income'], strategy='fill', imputation_method='median')\n",
    "        input_df['Gender'] = handle_missing_values(input_df, columns=['Gender'], strategy='fill', imputation_method='mode')\n",
    "    except Exception as e:\n",
    "        print(f\"Error in handling missing values: {str(e)}\")\n",
    "\n",
    "    # 3. Handle outliers\n",
    "    try:\n",
    "        input_df = filter_outliers(input_df, col_series=['Area Income'], method='iqr')\n",
    "    except Exception as e:\n",
    "        print(f\"Error in filtering outliers: {str(e)}\")\n",
    "\n",
    "    # 4. Feature encoding\n",
    "    try:\n",
    "        input_df, expected_columns = feature_encoding(input_df, original_data=ori_df_preprocessed)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in feature encoding: {str(e)}\")\n",
    "        if 'input_df' in locals():\n",
    "            print(f\"Input data columns: {input_df.columns}\")\n",
    "        print(f\"Original data columns: {ori_df_preprocessed.columns}\")\n",
    "\n",
    "    # 5. Feature scaling\n",
    "    try:\n",
    "        input_df = feature_scaling(input_df, original_data=ori_df_preprocessed)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in feature scaling: {str(e)}\")\n",
    "\n",
    "    # Return the preprocessed data\n",
    "    return input_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
